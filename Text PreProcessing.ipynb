{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task B: Text Pre-Processing\n",
    "#### Name: Rohit Sanjay Tapas\n",
    "\n",
    "Environment: Python 3 and Jupyter notebook\n",
    "\n",
    "Libraries used: \n",
    "* tika - used to parse pdf file to txt format\n",
    "* nltk - natural language toolkit (tokenizer, lemmatizer, collocations)\n",
    "* re (for regular expression, included in Anaconda Python 3) \n",
    "* itertools\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "\n",
    "* Main motive of this task is to convert extracted data into formatted data. The textual information or data is converted into its numerical representation.\n",
    "* The dataset consists of information of units offered at Monash University.\n",
    "* The data provided per unit includes:\n",
    "1.Unit Code\n",
    "2.Synopsis\n",
    "3.Outcomes\n",
    "* The task is to extract the information about each unit and generate a vector-space model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.  Import libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tika import parser\n",
    "import re\n",
    "import nltk\n",
    "from nltk.collocations import *\n",
    "from itertools import chain\n",
    "from nltk.tokenize import RegexpTokenizer \n",
    "from nltk.tokenize import MWETokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Convert data from pdf to txt\n",
    "\n",
    "* Tika library has been used to extract data from pdf file and convert it into txt format.\n",
    "* Enter the filename into parser.fromfile() to extract the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = parser.from_file('29812135.pdf')               #parse data from pdf file\n",
    "dataset = raw['content']                             #store parsed data in variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 File operations\n",
    "* Read all stop words from the stopwords file provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = open('stopwords_en.txt','r')                     #open stop words file\n",
    "stop = stop_words.readlines()                                 #read all the data in stopwords file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Initlialise lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = []                    #stores all tokens\n",
    "data = []                      #stores extracted data from file\n",
    "fin = []                       #list of lists which stores data unit wise\n",
    "uni_voc = []                   \n",
    "unigrams = []                  #list of unigrams\n",
    "stops=[]                       #list of stopwords                              \n",
    "stopped_tokens = []            #list of tokens after removing stop words\n",
    "init_vocab = []\n",
    "vocab = []                     #list of vocabulary\n",
    "stemmed_vocab = []             #list of stemmed vocab\n",
    "unit_code = []                 #list of unit codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Extract data unit wise\n",
    "\n",
    "* Regex is used to extract data unit wise.\n",
    "* Extracted data is then stored in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_filter = re.findall('([A-Z][A-Z][A-Z]\\d\\d\\d\\d)(.*?)]|[A-Z][A-Z][A-Z][A-Z]\\d\\d\\d\\d(.*?)]',dataset,re.DOTALL|re.MULTILINE)  #regex to extract data per unit\n",
    "for every in main_filter:\n",
    "    data.append(every)\n",
    "for each in data:                                      #storing data per unit as a list in a list ehich contains all the units.\n",
    "    for x in each:\n",
    "        each = x.replace('\\n',' ')\n",
    "        fin.append(each)\n",
    "del fin[2::3]\n",
    "unit_code.append(fin[0::2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Tokenization\n",
    "* The data is tokenized using the regex provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r\"\\w+(?:[-.]\\w+)?\")        #tokenise all the words\n",
    "for each in fin:\n",
    "    unigram_tokens = tokenizer.tokenize(each)\n",
    "    unigrams.append(unigram_tokens)\n",
    "\n",
    "for each in unigrams:\n",
    "    unique = list(set(each))                           #remove repeating tokens\n",
    "    uni_voc.append(unique)\n",
    "    \n",
    "mwe_tokenizer = MWETokenizer(uni_voc)\n",
    "for each in unigrams:\n",
    "    mwe_tokens = mwe_tokenizer.tokenize(each)\n",
    "    tokens.append(mwe_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Removing Stop Words\n",
    "* stop words provided are stored in a list\n",
    "* if the stop words occur in more than %95 of documents, they are removed\n",
    "* if length of token is less than 3, it is removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for each in stop:\n",
    "    each = each.replace('\\n',\"\")\n",
    "    stops.append(each)\n",
    "while len(tokens)!=len(stopped_tokens):\n",
    "    stopped_tokens.append([])\n",
    "\n",
    "for each in tokens:\n",
    "    init_vocab.append(each)\n",
    "\n",
    "for x in stops:                                        #remove stopwords from list of stopwords if with threshold set to %95\n",
    "    i = 0\n",
    "    for each in init_vocab:\n",
    "        if x in each:\n",
    "            i = i+1\n",
    "    if i>190:\n",
    "        stops.remove(x)\n",
    "\n",
    "i = 0\n",
    "for i in range(len(tokens)):                           #remove all stopwords from list which cantains lists of all units\n",
    "    for each in tokens[i]:\n",
    "        if each not in stops:\n",
    "            if len(each)>3:                            #remove tokens with length less than 3\n",
    "                each = each.lower()\n",
    "                stopped_tokens[i].append(each)\n",
    "\n",
    "\n",
    "for each in tokens:                                    #creating vocabular set which contains words from the whole dataset\n",
    "    for x in each:\n",
    "        if len(x)>3:\n",
    "            if x not in stops:\n",
    "                x = x.lower()\n",
    "                vocab.append(x)\n",
    "                \n",
    "temp_vocab = set(vocab)\n",
    "set_vocab = sorted(temp_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7 Removing rare tokens\n",
    "* Tokens are removed with the threshold set to %5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for each in temp_vocab:                                #filtering rare tokens from vocabulary which occur in less than %5 units.\n",
    "    i = 0\n",
    "    for x in stopped_tokens:\n",
    "        if each in x:\n",
    "            i = i+1\n",
    "    if i<10:\n",
    "        set_vocab.remove(each)\n",
    "\n",
    "set_vocab = set(set_vocab)\n",
    "set_vocab = sorted(set_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.8 Stemming\n",
    "* Tokens are stemmed using Porter stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()                                   #stemming all the tokens in vocabulary using porter stemmer\n",
    "for w in set_vocab:\n",
    "    stemmed_vocab.append(ps.stem(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.9 Finding Bigrams\n",
    "* Top 200 meaningful bigrams are found.\n",
    "* As we have to find meaningful bigrams, the bigrams which appear at least 4 times are taken and added to the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = list(chain.from_iterable(stopped_tokens))                                #finding first 200 meaningful bigrams\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "bigram_finder = nltk.collocations.BigramCollocationFinder.from_words(all_words)\n",
    "bigram_finder.apply_freq_filter(4)\n",
    "bigram_finder.apply_word_filter(lambda w: len(w) < 3)\n",
    "top_200_bigrams = bigram_finder.nbest(bigram_measures.pmi, 200)\n",
    "k = 0\n",
    "for each in top_200_bigrams:\n",
    "    stemmed_vocab.append(str(top_200_bigrams[k][0])+' '+str(top_200_bigrams[k][1]))     #adding the extracted bigrams to the final vocabulary\n",
    "    k = k+1\n",
    "stemmed_vocab = set(stemmed_vocab)\n",
    "stemmed_vocab = sorted(stemmed_vocab)                #sorting the final vocabulary alphabetically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('magnetic', 'resonance'),\n",
       " ('childhood', 'adolescence'),\n",
       " ('micro', 'nano'),\n",
       " ('businesses', 'operating'),\n",
       " ('non-fiction', 'fiction'),\n",
       " ('monash', 'university'),\n",
       " ('central', 'nervous'),\n",
       " ('male', 'female'),\n",
       " ('crime', 'prevention'),\n",
       " ('finite', 'element'),\n",
       " ('compare', 'contrast'),\n",
       " ('female', 'pelvis'),\n",
       " ('geological', 'maps'),\n",
       " ('korean', 'peninsula'),\n",
       " ('renewable', 'energy'),\n",
       " ('virtual', 'space'),\n",
       " ('infectious', 'diseases'),\n",
       " ('pacific', 'region'),\n",
       " ('arguments', 'existence'),\n",
       " ('female', 'reproductive'),\n",
       " ('corporate', 'finance'),\n",
       " ('decision', 'making'),\n",
       " ('delivery', 'platforms'),\n",
       " ('affecting', 'businesses'),\n",
       " ('dispute', 'resolution'),\n",
       " ('radiation', 'therapy'),\n",
       " ('buyer', 'behaviour'),\n",
       " ('sale', 'goods'),\n",
       " ('problem', 'solving'),\n",
       " ('public', 'sector'),\n",
       " ('indigenous', 'peoples'),\n",
       " ('nervous', 'system'),\n",
       " ('physical', 'computing'),\n",
       " ('individual', 'summative'),\n",
       " ('rules', 'occupational'),\n",
       " ('goods', 'services'),\n",
       " ('summative', 'assessment'),\n",
       " ('construction', 'disputes'),\n",
       " ('structure', 'function'),\n",
       " ('human', 'rights'),\n",
       " ('acquisition', 'comprehensive'),\n",
       " ('occupational', 'therapy'),\n",
       " ('beginner', 'level'),\n",
       " ('european', 'cinema'),\n",
       " ('physical', 'meteorology'),\n",
       " ('operating', 'global'),\n",
       " ('popular', 'culture'),\n",
       " ('safety', 'discipline'),\n",
       " ('topics', 'covered'),\n",
       " ('high', 'level'),\n",
       " ('real', 'world'),\n",
       " ('source', 'texts'),\n",
       " ('data', 'warehousing'),\n",
       " ('financial', 'risks'),\n",
       " ('patient', 'care'),\n",
       " ('infectious', 'disease'),\n",
       " ('primary', 'sources'),\n",
       " ('critical', 'thinking'),\n",
       " ('qualitative', 'quantitative'),\n",
       " ('introductory', 'level'),\n",
       " ('literature', 'review'),\n",
       " ('oral', 'written'),\n",
       " ('theoretical', 'methodological'),\n",
       " ('quantitative', 'qualitative'),\n",
       " ('framework', 'methodology'),\n",
       " ('theories', 'introductory'),\n",
       " ('social', 'theoretic'),\n",
       " ('production', 'consumption'),\n",
       " ('business', 'intelligence'),\n",
       " ('health', 'illness'),\n",
       " ('data', 'collection'),\n",
       " ('wide', 'range'),\n",
       " ('case', 'studies'),\n",
       " ('topics', 'include'),\n",
       " ('resolution', 'construction'),\n",
       " ('information', 'technology'),\n",
       " ('scientific', 'basis'),\n",
       " ('text', 'analysis'),\n",
       " ('local', 'global'),\n",
       " ('digital', 'video'),\n",
       " ('reproductive', 'systems'),\n",
       " ('electronic', 'business'),\n",
       " ('preservation', 'techniques'),\n",
       " ('health', 'safety'),\n",
       " ('dietetic', 'practice'),\n",
       " ('written', 'oral'),\n",
       " ('theoretical', 'empirical'),\n",
       " ('marketing', 'strategy'),\n",
       " ('english', 'international'),\n",
       " ('business', 'transactions'),\n",
       " ('comprehensive', 'case'),\n",
       " ('financial', 'planning'),\n",
       " ('apply', 'rules'),\n",
       " ('issues', 'affecting'),\n",
       " ('visual', 'practices'),\n",
       " ('risk', 'management'),\n",
       " ('organisational', 'communication'),\n",
       " ('communicate', 'effectively'),\n",
       " ('assessment', 'children'),\n",
       " ('infrastructure', 'systems'),\n",
       " ('individual', 'group'),\n",
       " ('management', 'accounting'),\n",
       " ('culture', 'society'),\n",
       " ('occupational', 'health'),\n",
       " ('scientific', 'literature'),\n",
       " ('element', 'analysis'),\n",
       " ('dealing', 'international'),\n",
       " ('demonstrate', 'proficiency'),\n",
       " ('public', 'health'),\n",
       " ('health', 'care'),\n",
       " ('translational', 'research'),\n",
       " ('broad', 'range'),\n",
       " ('good', 'understanding'),\n",
       " ('energy', 'systems'),\n",
       " ('engineering', 'systems'),\n",
       " ('personal', 'professional'),\n",
       " ('comprehensive', 'understanding'),\n",
       " ('demonstrate', 'good'),\n",
       " ('individual', 'team'),\n",
       " ('social', 'network'),\n",
       " ('capstone', 'unit'),\n",
       " ('skills', 'needed'),\n",
       " ('health', 'nutrition'),\n",
       " ('aims', 'develop'),\n",
       " ('related', 'assessment'),\n",
       " ('contemporary', 'visual'),\n",
       " ('national', 'international'),\n",
       " ('software', 'applications'),\n",
       " ('population', 'health'),\n",
       " ('studio', 'practice'),\n",
       " ('social', 'change'),\n",
       " ('social', 'institutions'),\n",
       " ('digital', 'media'),\n",
       " ('chinese', 'business'),\n",
       " ('multimedia', 'design'),\n",
       " ('unit', 'introduce'),\n",
       " ('critically', 'reflect'),\n",
       " ('this', 'unit'),\n",
       " ('unit', 'introduces'),\n",
       " ('data', 'types'),\n",
       " ('language', 'culture'),\n",
       " ('work', 'effectively'),\n",
       " ('international', 'commercial'),\n",
       " ('students', 'gain'),\n",
       " ('independent', 'research'),\n",
       " ('interface', 'design'),\n",
       " ('written', 'communication'),\n",
       " ('introduces', 'students'),\n",
       " ('critically', 'evaluate'),\n",
       " ('introduce', 'students'),\n",
       " ('global', 'context'),\n",
       " ('legal', 'system'),\n",
       " ('unit', 'examines'),\n",
       " ('unit', 'aims'),\n",
       " ('issues', 'related'),\n",
       " ('students', 'opportunity'),\n",
       " ('qualitative', 'data'),\n",
       " ('data', 'analysis'),\n",
       " ('students', 'undertaking'),\n",
       " ('develop', 'analytical'),\n",
       " ('economic', 'social'),\n",
       " ('discipline', 'practice'),\n",
       " ('unit', 'focuses'),\n",
       " ('demonstrate', 'ability'),\n",
       " ('design', 'bridge'),\n",
       " ('history', 'cultural'),\n",
       " ('critically', 'examine'),\n",
       " ('unit', 'explores'),\n",
       " ('analytical', 'skills'),\n",
       " ('addition', 'students'),\n",
       " ('disease', 'processes'),\n",
       " ('specific', 'issues'),\n",
       " ('social', 'political'),\n",
       " ('unit', 'builds'),\n",
       " ('international', 'business'),\n",
       " ('skills', 'acquired'),\n",
       " ('business', 'environment'),\n",
       " ('design', 'architecture'),\n",
       " ('skills', 'required'),\n",
       " ('critical', 'analysis'),\n",
       " ('unit', 'covers'),\n",
       " ('students', 'introduced'),\n",
       " ('reflect', 'critically'),\n",
       " ('analyse', 'interpret'),\n",
       " ('empirical', 'research'),\n",
       " ('marketing', 'theory'),\n",
       " ('theories', 'concepts'),\n",
       " ('system', 'including'),\n",
       " ('design', 'implementation'),\n",
       " ('qualitative', 'research'),\n",
       " ('critically', 'analyse'),\n",
       " ('unit', 'designed'),\n",
       " ('language', 'learning'),\n",
       " ('professional', 'development'),\n",
       " ('students', 'expected'),\n",
       " ('basic', 'theories'),\n",
       " ('research', 'frameworks'),\n",
       " ('marketing', 'strategies'),\n",
       " ('research', 'question'),\n",
       " ('legal', 'problems')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_200_bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we can see the first 200 meaningful bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.10 Creating vocab.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_dict = {}                                      #creating a dictionary for all tokens in the vocabulary with serial indices\n",
    "index = 1 \n",
    "for item in stemmed_vocab:\n",
    "        vocab_dict[item] = index\n",
    "        index = index+1\n",
    "for key in vocab_dict:\n",
    "    print(str(key)+ ' : ' + str(vocab_dict[key]),file = open(\"29812135_vocab.txt\",'a'))   #printing the final vocabulary to output file\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'abil': 1,\n",
       " 'academ': 2,\n",
       " 'acquir': 3,\n",
       " 'acquisition comprehensive': 4,\n",
       " 'activ': 5,\n",
       " 'addition students': 6,\n",
       " 'advanc': 7,\n",
       " 'affecting businesses': 8,\n",
       " 'aim': 9,\n",
       " 'aims develop': 10,\n",
       " 'analys': 11,\n",
       " 'analyse interpret': 12,\n",
       " 'analysi': 13,\n",
       " 'analyt': 14,\n",
       " 'analytical skills': 15,\n",
       " 'appli': 16,\n",
       " 'applic': 17,\n",
       " 'apply rules': 18,\n",
       " 'appreci': 19,\n",
       " 'approach': 20,\n",
       " 'area': 21,\n",
       " 'argument': 22,\n",
       " 'arguments existence': 23,\n",
       " 'articul': 24,\n",
       " 'aspect': 25,\n",
       " 'assess': 26,\n",
       " 'assessment children': 27,\n",
       " 'audienc': 28,\n",
       " 'australian': 29,\n",
       " 'awar': 30,\n",
       " 'base': 31,\n",
       " 'basi': 32,\n",
       " 'basic': 33,\n",
       " 'basic theories': 34,\n",
       " 'beginner level': 35,\n",
       " 'behaviour': 36,\n",
       " 'broad': 37,\n",
       " 'broad range': 38,\n",
       " 'busi': 39,\n",
       " 'business environment': 40,\n",
       " 'business intelligence': 41,\n",
       " 'business transactions': 42,\n",
       " 'businesses operating': 43,\n",
       " 'buyer behaviour': 44,\n",
       " 'capstone unit': 45,\n",
       " 'care': 46,\n",
       " 'case': 47,\n",
       " 'case studies': 48,\n",
       " 'central nervous': 49,\n",
       " 'challeng': 50,\n",
       " 'chang': 51,\n",
       " 'childhood adolescence': 52,\n",
       " 'chinese business': 53,\n",
       " 'chosen': 54,\n",
       " 'clinic': 55,\n",
       " 'common': 56,\n",
       " 'commun': 57,\n",
       " 'communicate effectively': 58,\n",
       " 'compare contrast': 59,\n",
       " 'compet': 60,\n",
       " 'complet': 61,\n",
       " 'complex': 62,\n",
       " 'comprehens': 63,\n",
       " 'comprehensive case': 64,\n",
       " 'comprehensive understanding': 65,\n",
       " 'concept': 66,\n",
       " 'conceptu': 67,\n",
       " 'conduct': 68,\n",
       " 'construct': 69,\n",
       " 'construction disputes': 70,\n",
       " 'contemporari': 71,\n",
       " 'contemporary visual': 72,\n",
       " 'context': 73,\n",
       " 'control': 74,\n",
       " 'core': 75,\n",
       " 'corporate finance': 76,\n",
       " 'cover': 77,\n",
       " 'creativ': 78,\n",
       " 'crime prevention': 79,\n",
       " 'critic': 80,\n",
       " 'critical analysis': 81,\n",
       " 'critical thinking': 82,\n",
       " 'critically analyse': 83,\n",
       " 'critically evaluate': 84,\n",
       " 'critically examine': 85,\n",
       " 'critically reflect': 86,\n",
       " 'critiqu': 87,\n",
       " 'cultur': 88,\n",
       " 'culture society': 89,\n",
       " 'current': 90,\n",
       " 'data': 91,\n",
       " 'data analysis': 92,\n",
       " 'data collection': 93,\n",
       " 'data types': 94,\n",
       " 'data warehousing': 95,\n",
       " 'dealing international': 96,\n",
       " 'debat': 97,\n",
       " 'decision making': 98,\n",
       " 'deliveri': 99,\n",
       " 'delivery platforms': 100,\n",
       " 'demonstr': 101,\n",
       " 'demonstrate ability': 102,\n",
       " 'demonstrate good': 103,\n",
       " 'demonstrate proficiency': 104,\n",
       " 'describ': 105,\n",
       " 'design': 106,\n",
       " 'design architecture': 107,\n",
       " 'design bridge': 108,\n",
       " 'design implementation': 109,\n",
       " 'determin': 110,\n",
       " 'develop': 111,\n",
       " 'develop analytical': 112,\n",
       " 'dietetic practice': 113,\n",
       " 'digit': 114,\n",
       " 'digital media': 115,\n",
       " 'digital video': 116,\n",
       " 'disciplin': 117,\n",
       " 'discipline practice': 118,\n",
       " 'discuss': 119,\n",
       " 'diseas': 120,\n",
       " 'disease processes': 121,\n",
       " 'dispute resolution': 122,\n",
       " 'divers': 123,\n",
       " 'econom': 124,\n",
       " 'economic social': 125,\n",
       " 'effect': 126,\n",
       " 'electronic business': 127,\n",
       " 'element': 128,\n",
       " 'element analysis': 129,\n",
       " 'emphasi': 130,\n",
       " 'empirical research': 131,\n",
       " 'energy systems': 132,\n",
       " 'engag': 133,\n",
       " 'engineering systems': 134,\n",
       " 'english international': 135,\n",
       " 'environ': 136,\n",
       " 'ethic': 137,\n",
       " 'european cinema': 138,\n",
       " 'evalu': 139,\n",
       " 'event': 140,\n",
       " 'evid': 141,\n",
       " 'examin': 142,\n",
       " 'experi': 143,\n",
       " 'explain': 144,\n",
       " 'explor': 145,\n",
       " 'express': 146,\n",
       " 'factor': 147,\n",
       " 'female pelvis': 148,\n",
       " 'female reproductive': 149,\n",
       " 'field': 150,\n",
       " 'financi': 151,\n",
       " 'financial planning': 152,\n",
       " 'financial risks': 153,\n",
       " 'find': 154,\n",
       " 'finite element': 155,\n",
       " 'focu': 156,\n",
       " 'focus': 157,\n",
       " 'form': 158,\n",
       " 'formul': 159,\n",
       " 'framework': 160,\n",
       " 'framework methodology': 161,\n",
       " 'function': 162,\n",
       " 'fundament': 163,\n",
       " 'gain': 164,\n",
       " 'gener': 165,\n",
       " 'geological maps': 166,\n",
       " 'global': 167,\n",
       " 'global context': 168,\n",
       " 'good understanding': 169,\n",
       " 'goods services': 170,\n",
       " 'group': 171,\n",
       " 'health': 172,\n",
       " 'health care': 173,\n",
       " 'health illness': 174,\n",
       " 'health nutrition': 175,\n",
       " 'health safety': 176,\n",
       " 'high level': 177,\n",
       " 'histor': 178,\n",
       " 'histori': 179,\n",
       " 'history cultural': 180,\n",
       " 'human rights': 181,\n",
       " 'idea': 182,\n",
       " 'ident': 183,\n",
       " 'identifi': 184,\n",
       " 'impact': 185,\n",
       " 'implement': 186,\n",
       " 'implic': 187,\n",
       " 'import': 188,\n",
       " 'includ': 189,\n",
       " 'independ': 190,\n",
       " 'independent research': 191,\n",
       " 'indigenous peoples': 192,\n",
       " 'individu': 193,\n",
       " 'individual group': 194,\n",
       " 'individual summative': 195,\n",
       " 'individual team': 196,\n",
       " 'industri': 197,\n",
       " 'infectious disease': 198,\n",
       " 'infectious diseases': 199,\n",
       " 'inform': 200,\n",
       " 'information technology': 201,\n",
       " 'infrastructure systems': 202,\n",
       " 'integr': 203,\n",
       " 'interface design': 204,\n",
       " 'intern': 205,\n",
       " 'international business': 206,\n",
       " 'international commercial': 207,\n",
       " 'interpret': 208,\n",
       " 'introduc': 209,\n",
       " 'introduce students': 210,\n",
       " 'introduces students': 211,\n",
       " 'introduct': 212,\n",
       " 'introductory level': 213,\n",
       " 'investig': 214,\n",
       " 'involv': 215,\n",
       " 'issu': 216,\n",
       " 'issues affecting': 217,\n",
       " 'issues related': 218,\n",
       " 'knowledg': 219,\n",
       " 'korean peninsula': 220,\n",
       " 'laboratori': 221,\n",
       " 'languag': 222,\n",
       " 'language culture': 223,\n",
       " 'language learning': 224,\n",
       " 'learn': 225,\n",
       " 'legal': 226,\n",
       " 'legal problems': 227,\n",
       " 'legal system': 228,\n",
       " 'level': 229,\n",
       " 'limit': 230,\n",
       " 'literatur': 231,\n",
       " 'literature review': 232,\n",
       " 'local': 233,\n",
       " 'local global': 234,\n",
       " 'magnetic resonance': 235,\n",
       " 'main': 236,\n",
       " 'major': 237,\n",
       " 'make': 238,\n",
       " 'male female': 239,\n",
       " 'manag': 240,\n",
       " 'management accounting': 241,\n",
       " 'market': 242,\n",
       " 'marketing strategies': 243,\n",
       " 'marketing strategy': 244,\n",
       " 'marketing theory': 245,\n",
       " 'materi': 246,\n",
       " 'media': 247,\n",
       " 'method': 248,\n",
       " 'methodolog': 249,\n",
       " 'micro nano': 250,\n",
       " 'model': 251,\n",
       " 'monash university': 252,\n",
       " 'multimedia design': 253,\n",
       " 'nation': 254,\n",
       " 'national international': 255,\n",
       " 'natur': 256,\n",
       " 'nervous system': 257,\n",
       " 'non-fiction fiction': 258,\n",
       " 'number': 259,\n",
       " 'occup': 260,\n",
       " 'occupational health': 261,\n",
       " 'occupational therapy': 262,\n",
       " 'operating global': 263,\n",
       " 'opportun': 264,\n",
       " 'oral': 265,\n",
       " 'oral written': 266,\n",
       " 'organis': 267,\n",
       " 'organisational communication': 268,\n",
       " 'outcom': 269,\n",
       " 'pacific region': 270,\n",
       " 'part': 271,\n",
       " 'patient care': 272,\n",
       " 'perform': 273,\n",
       " 'person': 274,\n",
       " 'personal professional': 275,\n",
       " 'perspect': 276,\n",
       " 'physic': 277,\n",
       " 'physical computing': 278,\n",
       " 'physical meteorology': 279,\n",
       " 'place': 280,\n",
       " 'plan': 281,\n",
       " 'polici': 282,\n",
       " 'polit': 283,\n",
       " 'popular culture': 284,\n",
       " 'population health': 285,\n",
       " 'potenti': 286,\n",
       " 'power': 287,\n",
       " 'practic': 288,\n",
       " 'prepar': 289,\n",
       " 'present': 290,\n",
       " 'preservation techniques': 291,\n",
       " 'primari': 292,\n",
       " 'primary sources': 293,\n",
       " 'principl': 294,\n",
       " 'problem': 295,\n",
       " 'problem solving': 296,\n",
       " 'process': 297,\n",
       " 'produc': 298,\n",
       " 'product': 299,\n",
       " 'production consumption': 300,\n",
       " 'profession': 301,\n",
       " 'professional development': 302,\n",
       " 'program': 303,\n",
       " 'project': 304,\n",
       " 'provid': 305,\n",
       " 'public health': 306,\n",
       " 'public sector': 307,\n",
       " 'qualitative data': 308,\n",
       " 'qualitative quantitative': 309,\n",
       " 'qualitative research': 310,\n",
       " 'qualiti': 311,\n",
       " 'quantit': 312,\n",
       " 'quantitative qualitative': 313,\n",
       " 'radiation therapy': 314,\n",
       " 'rang': 315,\n",
       " 'real': 316,\n",
       " 'real world': 317,\n",
       " 'recognis': 318,\n",
       " 'reflect': 319,\n",
       " 'reflect critically': 320,\n",
       " 'relat': 321,\n",
       " 'related assessment': 322,\n",
       " 'relationship': 323,\n",
       " 'relev': 324,\n",
       " 'renewable energy': 325,\n",
       " 'report': 326,\n",
       " 'reproductive systems': 327,\n",
       " 'requir': 328,\n",
       " 'research': 329,\n",
       " 'research frameworks': 330,\n",
       " 'research question': 331,\n",
       " 'resolut': 332,\n",
       " 'resolution construction': 333,\n",
       " 'resourc': 334,\n",
       " 'respons': 335,\n",
       " 'result': 336,\n",
       " 'review': 337,\n",
       " 'right': 338,\n",
       " 'risk management': 339,\n",
       " 'role': 340,\n",
       " 'rule': 341,\n",
       " 'rules occupational': 342,\n",
       " 'safeti': 343,\n",
       " 'safety discipline': 344,\n",
       " 'sale goods': 345,\n",
       " 'scientif': 346,\n",
       " 'scientific basis': 347,\n",
       " 'scientific literature': 348,\n",
       " 'servic': 349,\n",
       " 'set': 350,\n",
       " 'situat': 351,\n",
       " 'skill': 352,\n",
       " 'skills acquired': 353,\n",
       " 'skills needed': 354,\n",
       " 'skills required': 355,\n",
       " 'social': 356,\n",
       " 'social change': 357,\n",
       " 'social institutions': 358,\n",
       " 'social network': 359,\n",
       " 'social political': 360,\n",
       " 'social theoretic': 361,\n",
       " 'societi': 362,\n",
       " 'softwar': 363,\n",
       " 'software applications': 364,\n",
       " 'solut': 365,\n",
       " 'sound': 366,\n",
       " 'sourc': 367,\n",
       " 'source texts': 368,\n",
       " 'specif': 369,\n",
       " 'specific issues': 370,\n",
       " 'standard': 371,\n",
       " 'state': 372,\n",
       " 'strategi': 373,\n",
       " 'structur': 374,\n",
       " 'structure function': 375,\n",
       " 'student': 376,\n",
       " 'students expected': 377,\n",
       " 'students gain': 378,\n",
       " 'students introduced': 379,\n",
       " 'students opportunity': 380,\n",
       " 'students undertaking': 381,\n",
       " 'studi': 382,\n",
       " 'studio': 383,\n",
       " 'studio practice': 384,\n",
       " 'summative assessment': 385,\n",
       " 'synthesis': 386,\n",
       " 'system': 387,\n",
       " 'system including': 388,\n",
       " 'team': 389,\n",
       " 'technic': 390,\n",
       " 'techniqu': 391,\n",
       " 'technolog': 392,\n",
       " 'text': 393,\n",
       " 'text analysis': 394,\n",
       " 'theoret': 395,\n",
       " 'theoretical empirical': 396,\n",
       " 'theoretical methodological': 397,\n",
       " 'theori': 398,\n",
       " 'theories concepts': 399,\n",
       " 'theories introductory': 400,\n",
       " 'thi': 401,\n",
       " 'think': 402,\n",
       " 'this unit': 403,\n",
       " 'time': 404,\n",
       " 'tool': 405,\n",
       " 'topic': 406,\n",
       " 'topics covered': 407,\n",
       " 'topics include': 408,\n",
       " 'translational research': 409,\n",
       " 'type': 410,\n",
       " 'understand': 411,\n",
       " 'undertak': 412,\n",
       " 'unit': 413,\n",
       " 'unit aims': 414,\n",
       " 'unit builds': 415,\n",
       " 'unit covers': 416,\n",
       " 'unit designed': 417,\n",
       " 'unit examines': 418,\n",
       " 'unit explores': 419,\n",
       " 'unit focuses': 420,\n",
       " 'unit introduce': 421,\n",
       " 'unit introduces': 422,\n",
       " 'utilis': 423,\n",
       " 'varieti': 424,\n",
       " 'virtual space': 425,\n",
       " 'visual': 426,\n",
       " 'visual practices': 427,\n",
       " 'way': 428,\n",
       " 'wide range': 429,\n",
       " 'work': 430,\n",
       " 'work effectively': 431,\n",
       " 'world': 432,\n",
       " 'write': 433,\n",
       " 'written': 434,\n",
       " 'written communication': 435,\n",
       " 'written oral': 436}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Here we have our complete stemmed vocabulary.\n",
    "* for each token, we assign an index which will be used in creating the sparse vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.11 Creating sparse vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "\n",
    "for i in range(1,len(fin),2):                              #creating sparse vector and writing to output file\n",
    "    print('\\n',file = open(\"29812135_countVec.txt\",\"a\"))\n",
    "    print(unit_code[0][0], end=\",\",file = open(\"29812135_countVec.txt\",\"a\"))\n",
    "    for every in vocab_dict:\n",
    "        if every in fin[i]:   \n",
    "            print((str(vocab_dict[every]) + ':'+ str(fin[i].count(every))), end=\",\",file = open(\"29812135_countVec.txt\",\"a\"))\n",
    "            \n",
    "            \n",
    "    if unit_code:\n",
    "        del unit_code[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Above we can see the sparse vector file.\n",
    "* It should be interpreted in the following manner:\n",
    "* UnitCode, token1_index:wordcount, token2_index:wordcount..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
